{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOba8sypV/GAVJj/hR38vlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogis0/LLM_Labs/blob/master/LLM_Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1"
      ],
      "metadata": {
        "id": "CPEkx-5Q0mAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "huggingface_model = \"Gensyn/Qwen2.5-0.5B-Instruct\" # microsoft/Phi-3-mini-4k-instruct openai-community/gpt2 TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "login(token=\"<HF_TOKEN>\")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(huggingface_model, torch_dtype=torch.float32, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(huggingface_model, trust_remote_code=True)\n"
      ],
      "metadata": {
        "id": "7eSQG9opYgYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def get_all_weights(model):\n",
        "    weights = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad and 'weight' in name:\n",
        "            weights.append(param.data.view(-1))\n",
        "    return torch.cat(weights)\n",
        "\n",
        "original_weights = get_all_weights(model)\n",
        "\n",
        "def print_model_size(mdl):\n",
        "    torch.save(mdl.state_dict(), \"temp_weights.pt\")\n",
        "    size_mb = os.path.getsize(\"temp_weights.pt\") / float(2**20)\n",
        "    print(f\"Model size: {size_mb:.2f} MB\")\n",
        "    os.remove(\"temp_weights.pt\")"
      ],
      "metadata": {
        "id": "dp0yzdDbYgTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(original_weights.cpu().type(torch.float32).numpy(), bins=1500, color='skyblue')\n",
        "plt.title(\"Original Model Weight Distribution\")\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.xlim([-0.2, 0.2])\n",
        "plt.show()\n",
        "print_model_size(model)\n"
      ],
      "metadata": {
        "id": "eTICuiMbYgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "original_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "story_prompt = \"Finish the story: Once upon a time there was a boy\"\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": story_prompt,\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"return_full_text\": False\n",
        "}\n",
        "\n",
        "original_result = original_pipe(messages, **generation_args)\n",
        "original_response_pipe = original_result[0]['generated_text']\n",
        "\n",
        "print(f\"Original model response:\\n{original_response_pipe}\")\n",
        "del original_pipe\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "iEfvoutvUkTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Linear}, # layers\n",
        "    dtype=torch.qint8 # quantization type\n",
        ")\n",
        "\n",
        "del model\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bo61twOCYgHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weights = get_all_weights(quantized_model)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(quantized_weights.cpu().type(torch.float32).numpy(), bins=1500, color='orange')\n",
        "plt.title(\"Quantized Model Weights Distribution\")\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.xlim([-0.2, 0.2])\n",
        "plt.show()\n",
        "print_model_size(quantized_model)"
      ],
      "metadata": {
        "id": "vxM8y4_9Yf_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dequantized_model = torch.quantization.quantize_dynamic(\n",
        "    quantized_model,\n",
        "    {torch.nn.Linear}, # layers\n",
        "    dtype=torch.float32 # quantization type\n",
        ")\n",
        "\n",
        "dequantized_pipe = pipeline(\"text-generation\", model=dequantized_model, tokenizer=tokenizer)\n",
        "dequantized_result = dequantized_pipe(messages, **generation_args)\n",
        "dequantized_response_pipe = dequantized_result[0]['generated_text']\n",
        "\n",
        "print(f\"Quantized model response:\\n{dequantized_response_pipe}\")"
      ],
      "metadata": {
        "id": "j6Oklz4ZYf4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2"
      ],
      "metadata": {
        "id": "7SGGdxfU0q-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas sentence-transformers scikit-learn"
      ],
      "metadata": {
        "id": "gbQRqkCF1IEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "with open(\"paragraphs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    original_paragraphs = [p.strip() for p in f.read().split('\\n') if p.strip()]\n",
        "\n",
        "chunk_size = max(1, len(original_paragraphs) // 10)\n",
        "chunks = [' '.join(original_paragraphs[i:i + chunk_size]) for i in range(0, len(original_paragraphs), chunk_size)]\n",
        "\n",
        "chunk_vectors = model.encode(chunks)\n",
        "df = pd.DataFrame({'chunk': chunks, 'vectors': list(chunk_vectors)})\n",
        "\n",
        "with open(\"new_paragraphs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    new_paragraphs = [p.strip() for p in f.read().split('\\n') if p.strip()]\n",
        "\n",
        "new_vectors = model.encode(new_paragraphs)\n",
        "\n",
        "for i, new_para in enumerate(new_paragraphs):\n",
        "    # Cosine\n",
        "    distances = cosine_similarity([new_vectors[i]], chunk_vectors)[0]\n",
        "    best_idx = np.argmax(distances)\n",
        "\n",
        "    # Euclidean\n",
        "    # distances = euclidean_distances([new_vectors[i]], chunk_vectors)[0]\n",
        "    # best_idx = np.argmin(distances)\n",
        "\n",
        "    # Manhattan\n",
        "    # distances = manhattan_distances([new_vectors[i]], chunk_vectors)[0]\n",
        "    # best_idx = np.argmin(distances)\n",
        "\n",
        "    print(f\"\\n=== New Paragraph {i+1} ===\")\n",
        "    print(new_para)\n",
        "    print(\"\\n--- Most Similar Original Chunk ---\")\n",
        "    print(df.iloc[best_idx]['chunk'])\n",
        "    print(f\"\\n(Distance: {distances[best_idx]:.4f})\")\n"
      ],
      "metadata": {
        "id": "8g08_F59GNNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmC3WHa5I-wN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}